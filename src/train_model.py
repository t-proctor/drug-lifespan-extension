import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler # We might need StandardScaler if we didn't scale target
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import category_encoders as ce # For target encoding
import joblib # For saving models
import os
import sys

# --- Configuration ---
# Assuming running from workspace root
PROCESSED_DATA_PATH = 'data/processed/processed_drugage.pkl'
MODEL_OUTPUT_DIR = 'models'
RANDOM_STATE = 42
TEST_SIZE = 0.2

# Define target and feature columns based on the processed data
TARGET_COL = 'avg_lifespan_change_percent'
# Categorical features needing encoding
CATEGORICAL_FEATURES = ['compound_name', 'species', 'strain', 'gender', 'ITP']
# High cardinality feature needing special treatment (Target Encoding)
HIGH_CARDINALITY_FEATURES = ['compound_name']
# Low cardinality features for One-Hot Encoding
LOW_CARDINALITY_FEATURES = [f for f in CATEGORICAL_FEATURES if f not in HIGH_CARDINALITY_FEATURES]
# Numeric features (dosage_value is already scaled, dose_* are boolean/int)
# All dose_* columns generated by get_dummies
# We will identify numeric features dynamically after loading

# --- Main Function ---
def main():
    print(f"--- Starting Model Training --- ")
    print(f"Loading processed data from {PROCESSED_DATA_PATH}...")

    try:
        df = pd.read_pickle(PROCESSED_DATA_PATH)
    except FileNotFoundError:
        print(f"Error: Processed data file not found at {PROCESSED_DATA_PATH}. Run data processing first.", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Error loading pickle file: {e}", file=sys.stderr)
        sys.exit(1)

    print(f"Data loaded successfully. Shape: {df.shape}")

    # Identify numeric features (excluding target and already encoded dose_*)
    potential_numeric = df.select_dtypes(include=np.number).columns.tolist()
    dose_cols = [col for col in df.columns if col.startswith('dose_')]
    numeric_features = [col for col in potential_numeric if col != TARGET_COL and col not in dose_cols]
    print(f"Identified numeric features: {numeric_features}") # Should be just ['dosage_value']
    print(f"Identified low cardinality categorical features: {LOW_CARDINALITY_FEATURES}")
    print(f"Identified high cardinality categorical features: {HIGH_CARDINALITY_FEATURES}")
    print(f"Target column: {TARGET_COL}")

    # Define features (X) and target (y)
    X = df.drop(columns=[TARGET_COL])
    y = df[TARGET_COL]

    # Train/Test Split
    print(f"Splitting data into train/test sets (test_size={TEST_SIZE}, random_state={RANDOM_STATE})...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )
    print(f"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}")

    # --- Feature Encoding Pipeline ---
    # Define transformers for different column types
    # Note: dosage_value is already scaled; dose_* cols are already numeric/boolean

    # TargetEncoder for high cardinality features
    # Important: TargetEncoder must be fit ONLY on training data to prevent leakage
    # Using library 'category_encoders' as it handles this correctly
    target_encoder = ce.TargetEncoder(cols=HIGH_CARDINALITY_FEATURES, handle_missing='value', handle_unknown='value')

    # OneHotEncoder for low cardinality features
    # handle_unknown='ignore' ensures that if a category appears in test but not train, it gets all zeros
    one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

    # Create preprocessor using ColumnTransformer
    # Only apply transformers to the specified categorical columns
    # 'passthrough' means other columns (numeric, dose_*) are kept as is
    preprocessor = ColumnTransformer(
        transformers=[
            ('target_enc', target_encoder, HIGH_CARDINALITY_FEATURES),
            ('one_hot', one_hot_encoder, LOW_CARDINALITY_FEATURES)
        ],
        remainder='passthrough' # Keep other columns (numeric features, dose_cols)
    )

    # --- Model Training --- 
    # Using HistGradientBoostingRegressor (faster and handles NaNs if any slip through)
    print("\n--- Training HistGradientBoostingRegressor --- ")
    hgb_model = Pipeline(steps=[
        ('preprocess', preprocessor),
        ('regressor', HistGradientBoostingRegressor(random_state=RANDOM_STATE))
    ])

    print("Fitting HGB model...")
    hgb_model.fit(X_train, y_train)
    print("HGB model fitted.")

    # --- Evaluation --- 
    print("\n--- Evaluating HGB Model ---")
    y_pred_hgb = hgb_model.predict(X_test)
    r2_hgb = r2_score(y_test, y_pred_hgb)
    rmse_hgb = np.sqrt(mean_squared_error(y_test, y_pred_hgb))
    print(f"HGB R-squared (Test): {r2_hgb:.4f}")
    print(f"HGB RMSE (Test): {rmse_hgb:.4f}")

    # --- SHAP Analysis --- 
    print("\n--- Calculating SHAP Values --- ")
    # Need to apply the preprocessor step separately before SHAP
    # SHAP needs the actual numeric data the model saw
    X_train_processed = hgb_model.named_steps['preprocess'].fit_transform(X_train, y_train) # Fit again on train
    X_test_processed = hgb_model.named_steps['preprocess'].transform(X_test)

    # Get feature names after transformation
    # Need to handle names generated by OneHotEncoder and keep passthrough names
    try:
        ohe_feature_names = hgb_model.named_steps['preprocess'].named_transformers_['one_hot'].get_feature_names_out(LOW_CARDINALITY_FEATURES)
        # Get names of columns that were passed through
        passthrough_features_indices = hgb_model.named_steps['preprocess'].transformers_[-1][2] # Get indices/names of remainder columns
        passthrough_feature_names = X_train.columns[passthrough_features_indices].tolist()
        processed_feature_names = HIGH_CARDINALITY_FEATURES + list(ohe_feature_names) + passthrough_feature_names
    except Exception as e:
        print(f"Warning: Could not reliably get feature names after preprocessing: {e}")
        processed_feature_names = None # Fallback

    # Use TreeExplainer for tree models
    print("Creating SHAP explainer...")
    explainer = shap.TreeExplainer(hgb_model.named_steps['regressor'])
    print("Calculating SHAP values for the test set...")
    shap_values = explainer.shap_values(X_test_processed)

    print("Generating SHAP summary plot...")
    # Create DataFrame for SHAP values for better plotting with feature names
    if processed_feature_names and len(processed_feature_names) == X_test_processed.shape[1]:
        X_test_processed_df = pd.DataFrame(X_test_processed, columns=processed_feature_names)
        shap.summary_plot(shap_values, X_test_processed_df, show=False)
    else:
        print("Plotting SHAP summary without inferred feature names.")
        shap.summary_plot(shap_values, X_test_processed, show=False)
    
    # Save the plot
    shap_plot_path = os.path.join(MODEL_OUTPUT_DIR, 'shap_summary_plot.png')
    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)
    plt.savefig(shap_plot_path, bbox_inches='tight')
    print(f"SHAP summary plot saved to {shap_plot_path}")
    plt.close() # Close the plot to prevent display in console if not needed

    # --- Save Model (Optional) --- 
    model_path = os.path.join(MODEL_OUTPUT_DIR, 'hgb_lifespan_model.joblib')
    print(f"\nSaving trained model pipeline to {model_path}...")
    joblib.dump(hgb_model, model_path)
    print("Model saved.")

    print("\n--- Model Training Script Finished ---")

if __name__ == "__main__":
    main() 